{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación y Uso de YOLO para Detección de Objetos\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este proyecto, exploramos y evaluamos el rendimiento del modelo **YOLO (You Only Look Once)** para la detección de objetos en imágenes. Dado que YOLO utiliza una arquitectura ViT para la detección en imágen devolviendo bounding boxes, es ideal para empezar viendo el funcionamiento de modelos en hugging face\n",
    "\n",
    "El objetivo principal de este trabajo es comprobar la funcionalidad de YOLO y practicar su implementación en Jupiter Notebook para familiarizarnos con las herramientas\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Implementar y probar el modelo **YOLO** en imágenes de prueba.\n",
    "- Entrenar el modelo en un conjunto de datos propuesto y evaluar su rendimiento.\n",
    "- Practicar con todo ello implementaciones y evaluacion de modelos en Hugging Face\n",
    "\n",
    "¡Comencemos con la evaluación del modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import YolosForObjectDetection, YolosImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprobar el modelo**\n",
    "\n",
    "Para ello seguiremos los siguientes pasos\n",
    "\n",
    "1. Elegimos un dataset apropiado\n",
    "2. Procesamos los datos para cumplir con el formato de entrada del modelo\n",
    "3. Entrenamos el modelo\n",
    "4. Evaluamos el rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Elegir un dataste apropiado\n",
    "\n",
    "En este caso usaremos COCO, es un dataset de imágenes muy utilizado y escojemos este ya que es el usado por los propios creadores del modelo para evaluar su rendimiento, por lo que trataremos de imitar su experimento\n",
    "\n",
    "COCO tiene una estructura particular en la que la información de cada imagen viene en un JSON, los detalles del formato se pueden encontrar [aquí](https://cocodataset.org/#format-data). Pero esencialmente buscamos crear un diccionario con las imagenes, id, bbx y clase para poder cargarlo en el Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco train annotations: {'segmentation': [[239.97, 260.24, 222.04, 270.49, 199.84, 253.41, 213.5, 227.79, 259.62, 200.46, 274.13, 202.17, 277.55, 210.71, 249.37, 253.41, 237.41, 264.51, 242.54, 261.95, 228.87, 271.34]], 'area': 2765.1486500000005, 'iscrowd': 0, 'image_id': 558840, 'bbox': [199.84, 200.46, 77.71, 70.88], 'category_id': 58, 'id': 156}\n",
      "coco train images: {'license': 3, 'file_name': '000000391895.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000391895.jpg', 'height': 360, 'width': 640, 'date_captured': '2013-11-14 11:18:45', 'flickr_url': 'http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg', 'id': 391895}\n"
     ]
    }
   ],
   "source": [
    "from utils import COCODataProcessor\n",
    "COCO_DIR = 'datasets/COCO_2017'\n",
    "\n",
    "coco_data = COCODataProcessor(f'{COCO_DIR}/annotations/instances_train2017.json', \n",
    "                              f'{COCO_DIR}/train2017')\n",
    "\n",
    "print(f\"coco train annotations: {coco_data.json['annotations'][0]}\")\n",
    "print(f\"coco train images: {coco_data.json['images'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos el diccionario con el formato deseado pasamos a convertirlo en un dataset usando als funciones de torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = coco_data.image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar tenemos una cantiadad suficiente de datos por lo que ya que este ejercicio es para familiarizarnos con el proceso vamoa a partir de este dataset coger unos splits de train test y validation mucho menores para ver el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117266, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_SIZE = 10000\n",
    "VAL_SIZE = 2000\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "# Convertimos el dataset en un array de indices de imagenes para hacer el split mas eficientemente\n",
    "indices = np.arange(len(coco_data.image_dict.keys()))\n",
    "\n",
    "# Obtenemos la cantidad de imagenes con las que queremos trabajar aleatriamente\n",
    "selected_indices = np.random.choice(indices, size=TRAIN_SIZE+VAL_SIZE+TEST_SIZE, replace=False)\n",
    "\n",
    "# Separamos Test del conjunto de imagenes\n",
    "train_val_indices, test_indices = train_test_split(indices, test_size=TEST_SIZE, random_state=123)\n",
    "\n",
    "# Separamos validation de train y terminamos con los 3 conjuntos\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=VAL_SIZE, random_state=123)\n",
    "\n",
    "# Extraemos los subconjuntos correspondientes a cada división\n",
    "train_split = {key: coco_data.image_dict[key] for key in train_indices}\n",
    "val_split = {key: coco_data.image_dict[key] for key in val_indices}\n",
    "test_split = {key: coco_data.image_dict[key] for key in test_indices}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creados los splits vamos a crear los tensores con la información en formato de YOLO.\n",
    "\n",
    "Este modelo utiliza de **input las imágenes** por el contrario de **output** tenemos las **bounding boxes y sus clases de COCO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import COCOImageDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "dtrain = COCOImageDataset(train_split)\n",
    "dval = COCOImageDataset(val_split)\n",
    "\n",
    "train_loader = DataLoader(dtrain, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dval, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-base')\n",
    "processor = YolosImageProcessor.from_pretrained('hustvl/yolos-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos pasar a la fase de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()   \u001b[38;5;66;03m# ponemos el modelo en modo entrenamiento\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.train()   # ponemos el modelo en modo entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_scheduler\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Crea dataloaders\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Configura optimizador y scheduler\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Crea dataloaders\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset[\"validation\"], batch_size=16)\n",
    "\n",
    "# Configura optimizador y scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader))\n",
    "\n",
    "# Entrenamiento básico\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):  # Número de épocas\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "polyp_vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
